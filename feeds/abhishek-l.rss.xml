<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Often Bearing Metaphoric Tense</title><link>http://theanalyst.github.io/</link><description></description><atom:link href="http://theanalyst.github.io/feeds/abhishek-l.rss.xml" rel="self"></atom:link><lastBuildDate>Sun, 01 Feb 2015 00:00:00 +0530</lastBuildDate><item><title>Faster python builds in Travis with container based infra</title><link>http://theanalyst.github.io/faster-python-builds-in-travis-with-container-based-infra.html</link><description>&lt;p&gt;Lately, travis CI has started supporting builds using &lt;a href="http://blog.travis-ci.com/2014-12-17-faster-builds-with-container-based-infrastructure/"&gt;container&lt;/a&gt;
based infrastructure which run much faster due to more available
resources and good use of caching. This is currently possible only for
projects that don't use &lt;code&gt;sudo&lt;/code&gt; atm . For more details read &lt;a href="http://docs.travis-ci.com/user/workers/container-based-infrastructure/"&gt;this&lt;/a&gt;
post. Of late, I have been submitting Pull Requests to various
projects to use this feature wherever possible. This post is primarily
oriented towards Python projects.&lt;/p&gt;
&lt;p&gt;For many python projects, a significant amount of time is spent in
installing &amp;amp; building the dependencies. So caching these should save
you a good amount of time. Pip downloads can be cached specifying a
cache directory, which will avoid hitting the pypi mirror for every
line in your requirements. The below examples assume that your testing
script is something like &lt;code&gt;make travis&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;sudo&lt;/span&gt; &lt;span class="p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;false&lt;/span&gt;
&lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;cache&lt;/span&gt;&lt;span class="p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="l-Scalar-Plain"&gt;directories&lt;/span&gt;&lt;span class="p-Indicator"&gt;:&lt;/span&gt; 
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;$HOME/.pip-cache&lt;/span&gt;
&lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;script&lt;/span&gt;&lt;span class="p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;pip install -r requirements.txt --download-cache $HOME/.pip-cache&lt;/span&gt; 
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;make travis&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above lines will make sure that for subsequent runs pip packages
from the cache are used. However the packages will still need to be be
built, which means if you're using packages using C (lxml for eg), a
lot of time will be still spent for building the package itself. One
way of working around this would be the use of Python Wheels and
caching them as this avoids the need for subsequent builds. We could
download the wheels to a specified directory and cache that, so that
we can cut down on the build time.  So we would have something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;sudo&lt;/span&gt; &lt;span class="p-Indicator"&gt;:&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;false&lt;/span&gt;
&lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;cache&lt;/span&gt;&lt;span class="p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="l-Scalar-Plain"&gt;directories&lt;/span&gt;&lt;span class="p-Indicator"&gt;:&lt;/span&gt; 
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;$PWD/wheelhouse&lt;/span&gt;
&lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;script&lt;/span&gt;&lt;span class="p-Indicator"&gt;:&lt;/span&gt;
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;pip wheel -r requirements.txt&lt;/span&gt;
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;pip install -r requirements.txt&lt;/span&gt;
    &lt;span class="p-Indicator"&gt;-&lt;/span&gt; &lt;span class="l-Scalar-Plain"&gt;make travis&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And voila! We have faster travis builds.&lt;/p&gt;
&lt;p&gt;PS If you have better ways to speedup, I'll be happy to hear, please
let me know in comments or hit me up on &lt;a href="https://twitter.com/abhishekl"&gt;twitter&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Abhishek L</dc:creator><pubDate>Sun, 01 Feb 2015 00:00:00 +0530</pubDate><guid>tag:theanalyst.github.io,2015-02-01:faster-python-builds-in-travis-with-container-based-infra.html</guid><category>Python</category><category>Travis</category></item><item><title>2014: the year in books, using Goodreads, pandas &amp; Hy</title><link>http://theanalyst.github.io/2014-the-year-in-books-using-goodreads-pandas-hy.html</link><description>&lt;p&gt;&lt;code&gt;Note: The content has been updated to reflect the new kwargs style
in Hy, as such the snippets only work with the current Hy master&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;As 2014 is coming to a close, I thought it would be a nice time to
review the year as far as reading was concerned. Since I track the
books I read using Goodreads, I felt it would be an interesting
experiment to see some numbers from Goodreads. Though Goodreads
provides an api, it doesn't seem to be directly useful to get some
numbers from a user account (I could be wrong here). Fortunately, an
export to csv option is provieded in the account which helps in our
favor.&lt;/p&gt;
&lt;p&gt;For any kind of data analysis in python, &lt;a href="http://pandas.pydata.org"&gt;pandas&lt;/a&gt; is a brilliant
library. Also it does most of the heavy lifting as far as processing
csv files are concerned. Since &lt;a href="http://hylang.org"&gt;Hy&lt;/a&gt; works wherever python works, I
thought it would be an interesting experiment to use Hy to parse the
data.&lt;/p&gt;
&lt;p&gt;To start with reading csv is a simple call to pandas' &lt;code&gt;read_csv&lt;/code&gt;
function. This can be done with only the interesting fields. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;import &lt;/span&gt;&lt;span class="nv"&gt;pandas&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;numpy&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;seaborn&lt;/span&gt; &lt;span class="ss"&gt;:as&lt;/span&gt; &lt;span class="nv"&gt;sns&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;parse-goodreads-csv&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;required-fields&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Title&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Date Read&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Bookshelves&amp;quot;&lt;/span&gt;
                          &lt;span class="s"&gt;&amp;quot;Number of Pages&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Original Publication Year&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;pandas.read_csv&lt;/span&gt; &lt;span class="nv"&gt;filepath&lt;/span&gt; &lt;span class="ss"&gt;:usecols&lt;/span&gt; &lt;span class="nv"&gt;required-fields&lt;/span&gt;
                     &lt;span class="ss"&gt;:index-col&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Date Read&amp;quot;&lt;/span&gt; &lt;span class="ss"&gt;:parse-dates&lt;/span&gt; &lt;span class="nv"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now taking out only the column's we're interested in &amp;amp; filtering out
the data from only a particular year can be done by&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;books-in-year&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;dataframe&lt;/span&gt; &lt;span class="nv"&gt;year&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;day1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;fn &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="nv"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;-01-01&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))]]&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;slice&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;.&lt;/span&gt; &lt;span class="nv"&gt;dataframe&lt;/span&gt; &lt;span class="nv"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;day1&lt;/span&gt; &lt;span class="nv"&gt;year&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;day1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;inc &lt;/span&gt;&lt;span class="nv"&gt;year&lt;/span&gt;&lt;span class="p"&gt;)))))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;ix&lt;/code&gt; returns the index of dataframe, since we'll be indexing by date
read, this allows us to select the required range, by simply selecting
a date range from the beginning of the year to the next year.&lt;/p&gt;
&lt;p&gt;Since it would be interesting to see the count of books as well as the
pages on a monthly basis, we should filter the data on a monthly
basis. Pandas offers a &lt;a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html"&gt;&lt;code&gt;groupby&lt;/code&gt;&lt;/a&gt; &amp;amp; &lt;code&gt;aggregate&lt;/code&gt; much similiar to
SQL like queries. So all we have to do is to group pages by month
here.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;aggregate-by-month&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;dataframe&lt;/span&gt; &lt;span class="nv"&gt;params&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;Group a particular key by month&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;.groupby&lt;/span&gt; &lt;span class="nv"&gt;dataframe&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;.&lt;/span&gt; &lt;span class="nv"&gt;dataframe&lt;/span&gt; &lt;span class="nv"&gt;index&lt;/span&gt; &lt;span class="nv"&gt;month&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;.aggregate&lt;/span&gt; &lt;span class="nv"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above function assumes that the dataframe object is already
indexed by a timeframe data, (Date Read in our case), then a simple
groupby month is performed and supplied to &lt;code&gt;aggregate&lt;/code&gt; which does a
cumulative operation on the grouped data according the supplied
functions. The threading operator &lt;code&gt;-&amp;gt;&lt;/code&gt; makes the result of the first
call as the first argument of the function. Now we have enough to get
some stats for the year.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kd"&gt;defn &lt;/span&gt;&lt;span class="nv"&gt;process&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;let &lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;books-in-2014&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;parse-goodreads-csv&lt;/span&gt; &lt;span class="nv"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
              &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;books-in-year&lt;/span&gt; &lt;span class="mi"&gt;2014&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;pages-per-month&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;-&amp;gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;.&lt;/span&gt; &lt;span class="nv"&gt;books-in-2014&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Pages&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
                         &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;aggregate-by-month&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;sum&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;count&amp;quot;&lt;/span&gt; &lt;span class="nv"&gt;np.mean&lt;/span&gt;&lt;span class="p"&gt;]))]]&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Pages read in 2014 &amp;quot;&lt;/span&gt;
         &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nf"&gt;.&lt;/span&gt; &lt;span class="nv"&gt;books-in-2014&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Pages&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Pages read in kindle&amp;quot;&lt;/span&gt;
         &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nf"&gt;.&lt;/span&gt; &lt;span class="nv"&gt;books-in-2014&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nf"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;books-in-2014.Bookshelves&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;kindle&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
             &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number of Pages&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Monthly Stats&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="nv"&gt;pages-per-month&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First, the &lt;code&gt;parse-csv&lt;/code&gt; function is given only the interesting columns,
also indexing is done via date read, we get &lt;code&gt;pages-per-month&lt;/code&gt; via the
aggregate &amp;amp; groupby function shown earlier. Counting the pages read in
Kindle was done by summing columns containing the Book shelves
(alternatively if you had marked the editions correctly another field
suggest the Kindle edition..).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Pages read in 2014  6659.0
Pages read in kindle 4489.0
Monthly Stats
Number of Pages
            sum  count        mean
 1              1061      3  353.666667
 2               136      2   68.000000
 6               373      1  373.000000
 8               484      2  242.000000
 9              1445      8  180.625000
 10             2126     17  125.058824
 11              262      1  262.000000
 12              772      6  128.666667
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally this is the plot of pages&lt;/p&gt;
&lt;p&gt;&lt;img alt="pages_per_month" src="images/pages-per-month.png" /&gt;&lt;/p&gt;
&lt;p&gt;Overall 2014 was a good year as far as reading was concerned, read a
lot of classics which had been on my backlog for quite sometime. Also
read &lt;em&gt;The Guide&lt;/em&gt;, which was pending for quite some time. Next year I
should follow a more consistent schedule :), there were months with no
reading at all and a few with too much (the peak in october was
probably due to the combined holidays here, at that time) The raw data
and output (as well as the program) are available in my &lt;a href="http://github.com/theanalyst/cuddlebear"&gt;github&lt;/a&gt;
repo.&lt;/p&gt;
&lt;p&gt;PS For those of you interested in knowing what I've been reading
checkout &lt;a href="https://www.goodreads.com/review/list/14029645-abhishek?read_at=2014&amp;amp;view=covers"&gt;my goodreads 2014 list&lt;/a&gt;. Do suggest anything worth
reading in the comments. Cya!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Abhishek L</dc:creator><pubDate>Sun, 21 Dec 2014 00:00:00 +0530</pubDate><guid>tag:theanalyst.github.io,2014-12-21:2014-the-year-in-books-using-goodreads-pandas-hy.html</guid><category>Hy</category><category>Goodreads</category><category>Pandas</category><category>Kindle</category></item><item><title>Pelican updated!</title><link>http://theanalyst.github.io/pelican-updated.html</link><description>&lt;p&gt;It has been a significant time between the posts. This time the reason
moslty being me lazy enough not to fix the blog after a pelican
update. Surpisingly though there wasn't much of a change required
other than tweaking a couple of configuration variables &amp;amp; renaming the
&lt;code&gt;pelicanconf&lt;/code&gt; file. Hope to start blogging again soon. This time for
real and hopefully more tech posts. C ya :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Abhishek L</dc:creator><pubDate>Mon, 08 Sep 2014 00:00:00 +0530</pubDate><guid>tag:theanalyst.github.io,2014-09-08:pelican-updated.html</guid><category>Blog</category><category>Procrastination</category><category>Pelican</category></item><item><title>I'm Back (the hyperbole version)</title><link>http://theanalyst.github.io/im-back-the-hyperbole-version.html</link><description>&lt;p&gt;This is the quintessential “I'm back” post in blogosphere, which is
often marked by a blogger coming up with &lt;em&gt;imaginative&lt;/em&gt; reasons
to explain a questionable absence from the virtual world. 
Tempting explanations for, not writing for so long, include 
work pressure, often marked by people blaming work for
(not) following their interests, a generic term, which, writers &amp;amp; artists
lovingly call the writer’s block, to describe their shortage of
creativity. Of course, if you know me, you know the actual reason is
just a cover up for not mentioning my often recurring theme of
procrastination. This time though twitcrastination&lt;sup id="fnref:twit"&gt;&lt;a class="footnote-ref" href="#fn:twit" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt; is to blame
for most of it.&lt;/p&gt;
&lt;p&gt;Of late, I have found a new obsession for (micro) blogging in form of
twitter, a service, which apparently,&lt;s&gt; lets you post links to
instagram dinner pics&lt;/s&gt;, helps people to connect to people and
organisations they care about. However, what happens in reality is
explained in part by the graph below.(A certain Gauss must be
twisting &amp;amp; turning in his grave here, for the blatant abuse of normal
distributions that is about to follow &lt;sup id="fnref:xkcd"&gt;&lt;a class="footnote-ref" href="#fn:xkcd" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img alt="xkcd_productivity" src="images/productivity.png" /&gt; &lt;/p&gt;
&lt;p&gt;Seeing the world's desire to shorten its verbosity, and the disturbing
increase in TL;DR statuses, I am thinking of killling my blog to 
embrace the merry little challenges of microblogging. 
Why should anything stand the test of time and be etched in stone?
Its 2013, ephemeral (like twitter statuses) is the way to go for your thoughts and actions&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:twit"&gt;
&lt;p&gt;Procrastination enabled by use of twitter, a term which the 
twitterati obsessively uses to explain their apparent lack of productivity&amp;#160;&lt;a class="footnote-backref" href="#fnref:twit" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:xkcd"&gt;
&lt;p&gt;XKCD plots,  the cornerstone of any descriptive
stats, XKCDify any matplotlib graph; details &lt;a href="http://jakevdp.github.com/blog/2012/10/07/xkcd-style-plots-in-matplotlib"&gt;here&lt;/a&gt; and source &lt;a href="https://gist.github.com/theanalyst/5284221"&gt;here&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:xkcd" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:blog"&gt;
&lt;p&gt;line stolen from the title of a friend’s awesome &lt;a href="http://istheurlavailable.wordpress.com"&gt;blog&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:blog" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Abhishek L</dc:creator><pubDate>Mon, 01 Apr 2013 00:00:00 +0530</pubDate><guid>tag:theanalyst.github.io,2013-04-01:im-back-the-hyperbole-version.html</guid><category>Rants</category><category>Writing</category><category>Twitter</category></item><item><title>Hello World</title><link>http://theanalyst.github.io/hello-world.html</link><description>&lt;p&gt;If you're reading this, it means that I have successfully started
blogging again. As the more perceptive of you might have
realized, this blog isn't hosted at &lt;em&gt;blogger&lt;/em&gt; anymore well this new
avatar is powered by &lt;a href="pelican.notmyidea.org"&gt;pelican&lt;/a&gt;, an awesome
static blogging engine written in python and served by github pages,
(of course the second part should be intuitionally obvious, given the
url atm). &lt;/p&gt;
&lt;p&gt;While the sabbatical from &lt;em&gt;blogosphere&lt;/em&gt; has been long, it was more
because of lack of anything to post rather than too busy to post
anything, and now I'm back from a questionably long self-imposed
exile. And life has been relatively smooth in the interlude,(though my
last post was from college, which I dearly miss now) and I'm a
graduand now with a job going along just fine and learning new things
in the process.&lt;/p&gt;
&lt;p&gt;While I'm still in the process of importing previous posts and
comments from blogger (read I am yet to decide whether the pain is
worth it or not), you should have fun reading the blog with its
current content which should be narcissistic links to me-pages. You
should be seeing proper posts being pushed into this url soon.&lt;/p&gt;
&lt;p&gt;If you have read upto this point, I would urge you to take the leap of
faith, and follow my alter-egos in the parallel universes of
&lt;em&gt;twitterati&lt;/em&gt; and &lt;em&gt;blogosphere&lt;/em&gt;. &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Abhishek L</dc:creator><pubDate>Fri, 08 Jun 2012 00:00:00 +0530</pubDate><guid>tag:theanalyst.github.io,2012-06-08:hello-world.html</guid><category>Rants</category></item></channel></rss>